{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fef8810",
   "metadata": {},
   "source": [
    "# Advanced Feature Engineering: Aggregating and Analyzing Grocery Category Amounts with SQL, Python, and PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6936e",
   "metadata": {},
   "source": [
    "**Introduction:**\n",
    "\n",
    "In this comprehensive data science project, our main focus is to showcase the process of advanced feature engineering using a combination of PySpark, Python, and SQL. We aim to demonstrate how to extract valuable insights and create multiple relevant variables from a dataset representing grocery category amounts in a simple and approachable manner.\n",
    "\n",
    "\n",
    "**Data Description:**\n",
    "\n",
    "The dataset used in this project represents the shopping data of three clients over a three-month period. It provides detailed information on the amount of money spent by each client on various categories of grocery items. Each entry includes the client's ID, the period of purchase, and the corresponding amounts spent on categories such as bakery, produce, dairy, meat, frozen foods, canned goods, beverages, snacks, condiments, and grains. It is important to note that this dataset is a mock dataset created solely for the purpose of demonstrating feature engineering techniques and does not reflect actual supermarket data.\n",
    "\n",
    "\n",
    "**Project Goals:**\n",
    "\n",
    "The main objective of this project is to illustrate how feature engineering can create new variables that capture important information and patterns within the dataset. By leveraging PySpark, Python, and SQL, we aim to showcase the process of manipulating and transforming the data to generate these new variables. These variables have the potential to provide deeper insights into customer behavior and enhance the accuracy of predictive models.\n",
    "\n",
    "\n",
    "**Data Science Techniques:**\n",
    "\n",
    "To achieve our project goals, we will leverage the capabilities of PySpark, Python, and SQL. These powerful tools allow us to perform various feature engineering techniques, such as aggregations, calculations, and data manipulation. By combining these techniques, we can create new variables that capture relevant information from the original features of the dataset. This process will enable us to uncover hidden patterns, understand customer preferences, and improve the overall analysis.\n",
    "\n",
    "\n",
    "**Conclusions:** \n",
    "\n",
    "Please refer to the end of the project for detailed conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d13ad7",
   "metadata": {},
   "source": [
    "# Creating the Grocery Supermarket Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77721b0",
   "metadata": {},
   "source": [
    "In this data science portfolio project, we start by importing the necessary libraries and initializing Apache Spark using the \"findspark\" package. This ensures that Spark is ready to be used for our data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791c28e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\u632915\\\\.conda\\\\envs\\\\my-env\\\\lib\\\\site-packages\\\\pyspark'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c2542c",
   "metadata": {},
   "source": [
    "First, we define the schema for our dataset, specifying the column names and data types. It's important to note that the dataset used in this code is a mock dataset created solely for the purpose of demonstrating feature engineering techniques and does not reflect actual supermarket data.\n",
    "\n",
    "Then, we create a DataFrame named \"grocery_category_amount_db\" using the provided sample data and the defined schema. This DataFrame represents the grocery category amounts.\n",
    "\n",
    "Additionally, we create a temporary view of the DataFrame, allowing us to easily perform SQL-like queries on the data.\n",
    "\n",
    "Finally, we convert the PySpark DataFrame to a Python DataFrame using the toPandas() method. This conversion enables us to work with the data using familiar Python libraries and syntax, providing more flexibility and ease of analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da14f6c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIENT_ID</th>\n",
       "      <th>Period</th>\n",
       "      <th>Bakery_amount</th>\n",
       "      <th>Produce_amount</th>\n",
       "      <th>Dairy_amount</th>\n",
       "      <th>Meat_amount</th>\n",
       "      <th>Frozen_foods_amount</th>\n",
       "      <th>Canned_goods_amount</th>\n",
       "      <th>Beverages_amount</th>\n",
       "      <th>Snacks_amount</th>\n",
       "      <th>Condiments_amount</th>\n",
       "      <th>Grains_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>client_1</td>\n",
       "      <td>Jan-2023</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>250</td>\n",
       "      <td>220</td>\n",
       "      <td>180</td>\n",
       "      <td>300</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client_1</td>\n",
       "      <td>Feb-2023</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>client_1</td>\n",
       "      <td>Mar-2023</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>150</td>\n",
       "      <td>80</td>\n",
       "      <td>120</td>\n",
       "      <td>150</td>\n",
       "      <td>250</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>client_2</td>\n",
       "      <td>Jan-2023</td>\n",
       "      <td>1300</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>client_2</td>\n",
       "      <td>Feb-2023</td>\n",
       "      <td>1000</td>\n",
       "      <td>30</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>client_2</td>\n",
       "      <td>Mar-2023</td>\n",
       "      <td>1200</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>client_3</td>\n",
       "      <td>Jan-2023</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>client_3</td>\n",
       "      <td>Feb-2023</td>\n",
       "      <td>120</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>150</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>250</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>client_3</td>\n",
       "      <td>Mar-2023</td>\n",
       "      <td>0</td>\n",
       "      <td>220</td>\n",
       "      <td>180</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>160</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>200</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CLIENT_ID    Period  Bakery_amount  Produce_amount  Dairy_amount  \\\n",
       "0  client_1  Jan-2023            150             200             0   \n",
       "1  client_1  Feb-2023              0             150           150   \n",
       "2  client_1  Mar-2023              0             120            80   \n",
       "3  client_2  Jan-2023           1300              10             0   \n",
       "4  client_2  Feb-2023           1000              30            80   \n",
       "5  client_2  Mar-2023           1200              40            20   \n",
       "6  client_3  Jan-2023            200             150           150   \n",
       "7  client_3  Feb-2023            120             120            80   \n",
       "8  client_3  Mar-2023              0             220           180   \n",
       "\n",
       "   Meat_amount  Frozen_foods_amount  Canned_goods_amount  Beverages_amount  \\\n",
       "0           80                  250                  220               180   \n",
       "1          200                  150                  150               150   \n",
       "2          150                   80                  120               150   \n",
       "3            0                   90                    0                 0   \n",
       "4            0                    0                    0                80   \n",
       "5            0                    0                    0                 0   \n",
       "6          200                  150                  150               150   \n",
       "7          150                   80                    0               150   \n",
       "8          120                   90                  160               120   \n",
       "\n",
       "   Snacks_amount  Condiments_amount  Grains_amount  \n",
       "0            300                 90            120  \n",
       "1            150                200            100  \n",
       "2            250                150            150  \n",
       "3              0                  0              0  \n",
       "4              0                 80              0  \n",
       "5              0                  0              0  \n",
       "6            150                200            100  \n",
       "7            250                150            150  \n",
       "8             80                200            140  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Define the schema\n",
    "schema = StructType([\n",
    "    StructField('CLIENT_ID', StringType(), True),\n",
    "    StructField('Period', StringType(), True),\n",
    "    StructField('Bakery_amount', IntegerType(), True),\n",
    "    StructField('Produce_amount', IntegerType(), True),\n",
    "    StructField('Dairy_amount', IntegerType(), True),\n",
    "    StructField('Meat_amount', IntegerType(), True),\n",
    "    StructField('Frozen_foods_amount', IntegerType(), True),\n",
    "    StructField('Canned_goods_amount', IntegerType(), True),\n",
    "    StructField('Beverages_amount', IntegerType(), True),\n",
    "    StructField('Snacks_amount', IntegerType(), True),\n",
    "    StructField('Condiments_amount', IntegerType(), True),\n",
    "    StructField('Grains_amount', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a sample DataFrame with mock data\n",
    "data = [\n",
    "    ('client_1', 'Jan-2023', 150, 200, 0, 80, 250, 220, 180, 300, 90, 120),\n",
    "    ('client_1', 'Feb-2023', 0, 150, 150, 200, 150, 150, 150, 150, 200, 100),\n",
    "    ('client_1', 'Mar-2023', 0, 120, 80, 150, 80, 120, 150, 250, 150, 150),\n",
    "    ('client_2', 'Jan-2023', 1300, 10, 0, 0, 90, 0, 0, 0, 0, 0),\n",
    "    ('client_2', 'Feb-2023', 1000, 30, 80, 0, 0, 0, 80, 0, 80, 0),\n",
    "    ('client_2', 'Mar-2023', 1200, 40, 20, 0, 0, 0, 0, 0, 0, 0),\n",
    "    ('client_3', 'Jan-2023', 200, 150, 150, 200, 150, 150, 150, 150, 200, 100),\n",
    "    ('client_3', 'Feb-2023', 120, 120, 80, 150, 80, 0, 150, 250, 150, 150),\n",
    "    ('client_3', 'Mar-2023', 0, 220, 180, 120, 90, 160, 120, 80, 200, 140)\n",
    "]\n",
    "\n",
    "# Create the SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
    "\n",
    "# Create a database for grocery category amounts\n",
    "grocery_category_amount_db = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Create a temporary view\n",
    "grocery_category_amount_db.createOrReplaceTempView(\"grocery_category_amount\")\n",
    "\n",
    "# Show the database\n",
    "# grocery_category_amount_db.show(1, vertical=True)\n",
    "\n",
    "# Convert PySpark DataFrame to Python DataFrame\n",
    "python_df = grocery_category_amount_db.toPandas()\n",
    "python_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf5901c",
   "metadata": {},
   "source": [
    "As we can see above, the objective is to analyze the expenditure of each customer on different categories of grocery items. Each entry contains the customer's ID, the purchase period, and the corresponding expenses for categories including bakery, produce, dairy, meat, frozen foods, canned goods, beverages, snacks, condiments, and grains. It is essential to note that this dataset is small and fictitious, created exclusively for showcasing feature engineering techniques. It does not represent real supermarket data.\n",
    "Below, we will explore the process of aggregating grocery category amounts using PySpark for feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c85acc",
   "metadata": {},
   "source": [
    "# Feature Engineering: Aggregating Grocery Category Amounts with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cca0a4",
   "metadata": {},
   "source": [
    "First, we define the `calculate_total` function. This function accepts two parameters: `pVar` and `pName`. When calling the function as `calculate_total(pVar, pName)`, `pVar` represents a list of variables from the dataframe to which the function will be applied, while `pName` is the name assigned to the calculated field.\n",
    "\n",
    "The purpose of this function is to generate SQL code that calculates the sum of all fields in the dataset that end with the suffix \"_amount\". To achieve this, the function iterates through the elements in `pVar` and constructs a partial SQL statement using the `coalesce` function. This function handles any null values by replacing them with zero.\n",
    "\n",
    "The resulting SQL code is stored as a string in the `sql_total_amount` object. It includes the summed values of the relevant fields and assigns it an alias of \"total_\" followed by `pName`.\n",
    "\n",
    "In the code snippet provided, we first retrieve the first row of the dataset `grocery_category_amount_df` using the `spark.sql` function. This dataset likely contains information about grocery category amounts.\n",
    "\n",
    "Then, we call the `calculate_total` function with a list comprehension that extracts the column names ending with \"_amount\" from `grocery_category_amount_df.columns`, along with the category name 'amount'. The result is assigned to the `sql_total_amount` variable, which now holds the generated SQL code for calculating the total amount.\n",
    "\n",
    "Overall, this code demonstrates the process of generating SQL code for aggregating grocery category amounts, specifically by summing the corresponding fields and handling null values using `coalesce`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df072be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',SUM(coalesce(Bakery_amount,0)+coalesce(Produce_amount,0)+coalesce(Dairy_amount,0)+coalesce(Meat_amount,0)+coalesce(Frozen_foods_amount,0)+coalesce(Canned_goods_amount,0)+coalesce(Beverages_amount,0)+coalesce(Snacks_amount,0)+coalesce(Condiments_amount,0)+coalesce(Grains_amount,0)) as total_amount'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# Define the function\n",
    "def calculate_total(pVar, pName):\n",
    "    sql = \",\"\n",
    "    for h in pVar:\n",
    "        if sql == \",\":\n",
    "            sql += \"SUM(coalesce(\"+h+\",0)\"\n",
    "        else: \n",
    "            sql += \"+coalesce(\"+h+\",0)\"\n",
    "    sql += \") as total_\" + pName\n",
    "\n",
    "    return sql\n",
    "\n",
    "grocery_category_amount_df  = spark.sql(\"SELECT * FROM grocery_category_amount limit 1\")\n",
    "sql_total_amount = calculate_total([ x for x in grocery_category_amount_df.columns if x.endswith('_amount')], 'amount')\n",
    "sql_total_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c30870",
   "metadata": {},
   "source": [
    "We create a dataframe called `total_grocery_category_amount_df` by executing an SQL query. In the query, we utilize the previously defined `sql_total_amount` object and group the results by the `CLIENT_ID` field, which is declared as `key_field: key_field = 'CLIENT_ID'`.\n",
    "\n",
    "To execute the query, we use the `spark.sql()` function and pass it a multi-line SQL string. The string includes the `key_field` and `sql_total_amount` to select and calculate the total amount for each client. The `grocery_category_amount` table is used as the data source.\n",
    "\n",
    "After executing the query, we display the resulting dataframe using the `show()` method. Setting `vertical=True` allows for a vertical display of the dataframe, making it easier to view all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7f1a0d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------\n",
      " CLIENT_ID    | client_1 \n",
      " total_amount | 4240     \n",
      "-RECORD 1----------------\n",
      " CLIENT_ID    | client_2 \n",
      " total_amount | 3930     \n",
      "-RECORD 2----------------\n",
      " CLIENT_ID    | client_3 \n",
      " total_amount | 4160     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "key_field= 'CLIENT_ID'\n",
    "\n",
    "# Use the result in a SQL query\n",
    "total_grocery_category_amount_df = spark.sql(\"\"\"SELECT \"\"\" + key_field + sql_total_amount + \"\"\" FROM grocery_category_amount\n",
    "group by \"\"\"+ key_field)\n",
    "total_grocery_category_amount_df.show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474770b",
   "metadata": {},
   "source": [
    "As observed in the above output, client_1 has incurred a total expenditure of 4240 over the past 3 months. Similarly, client_2 has accumulated a total expense of 3930 during the same period, while client_3 has spent a total amount of 4160 over the last 3 months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc428037",
   "metadata": {},
   "source": [
    "# Feature Engineering: Calculating Multiple Statistics for Grocery Category Amounts with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c09d0",
   "metadata": {},
   "source": [
    "Similarly to the `calculate_total` function, we can create a `calculate_derivatives` function. This function takes one parameter, `pVar`, which represents a list of variables from the dataframe to which the function will be applied. The purpose of this function is to generate SQL code that calculates various statistics for each field in the dataset.\n",
    "\n",
    "In the `calculate_derivatives` function, we iterate through the elements in `pVar` and construct a SQL string. This string includes calculations for the average (`AVG`), sum (`SUM`), minimum (`MIN`), maximum (`MAX`), and count of periods where the value is greater than zero for each field. The `coalesce` function handles any null values by replacing them with zero.\n",
    "\n",
    "The resulting SQL code is returned as a string.\n",
    "\n",
    "In the provided code snippet, we first retrieve the first row of the `grocery_category_amount_df` dataset using the `spark.sql` function. This dataset likely contains information about grocery category amounts.\n",
    "\n",
    "Then, we call the `calculate_derivatives` function with a list comprehension that extracts the column names ending with \"_amount\" from `grocery_category_amount_df.columns`. The result is assigned to the `sql_derivatives` variable, which now holds the generated SQL code for calculating the desired statistics.\n",
    "\n",
    "Overall, this code showcases the creation of a function for generating SQL code to calculate various statistics, such as average, sum, minimum, maximum, and the count of periods with non-zero values for each field in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8353ba26",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "',AVG(coalesce(Bakery_amount,0)) as Bakery_amount_avg ,SUM(coalesce(Bakery_amount,0)) as Bakery_amount_sum ,min(coalesce(Bakery_amount,0)) as Bakery_amount_min ,max(coalesce(Bakery_amount,0)) as Bakery_amount_max  ,count(distinct case when coalesce(Bakery_amount,0) > 0 then Period end  ) as Bakery_amount_periods ,AVG(coalesce(Produce_amount,0)) as Produce_amount_avg ,SUM(coalesce(Produce_amount,0)) as Produce_amount_sum ,min(coalesce(Produce_amount,0)) as Produce_amount_min ,max(coalesce(Produce_amount,0)) as Produce_amount_max  ,count(distinct case when coalesce(Produce_amount,0) > 0 then Period end  ) as Produce_amount_periods ,AVG(coalesce(Dairy_amount,0)) as Dairy_amount_avg ,SUM(coalesce(Dairy_amount,0)) as Dairy_amount_sum ,min(coalesce(Dairy_amount,0)) as Dairy_amount_min ,max(coalesce(Dairy_amount,0)) as Dairy_amount_max  ,count(distinct case when coalesce(Dairy_amount,0) > 0 then Period end  ) as Dairy_amount_periods ,AVG(coalesce(Meat_amount,0)) as Meat_amount_avg ,SUM(coalesce(Meat_amount,0)) as Meat_amount_sum ,min(coalesce(Meat_amount,0)) as Meat_amount_min ,max(coalesce(Meat_amount,0)) as Meat_amount_max  ,count(distinct case when coalesce(Meat_amount,0) > 0 then Period end  ) as Meat_amount_periods ,AVG(coalesce(Frozen_foods_amount,0)) as Frozen_foods_amount_avg ,SUM(coalesce(Frozen_foods_amount,0)) as Frozen_foods_amount_sum ,min(coalesce(Frozen_foods_amount,0)) as Frozen_foods_amount_min ,max(coalesce(Frozen_foods_amount,0)) as Frozen_foods_amount_max  ,count(distinct case when coalesce(Frozen_foods_amount,0) > 0 then Period end  ) as Frozen_foods_amount_periods ,AVG(coalesce(Canned_goods_amount,0)) as Canned_goods_amount_avg ,SUM(coalesce(Canned_goods_amount,0)) as Canned_goods_amount_sum ,min(coalesce(Canned_goods_amount,0)) as Canned_goods_amount_min ,max(coalesce(Canned_goods_amount,0)) as Canned_goods_amount_max  ,count(distinct case when coalesce(Canned_goods_amount,0) > 0 then Period end  ) as Canned_goods_amount_periods ,AVG(coalesce(Beverages_amount,0)) as Beverages_amount_avg ,SUM(coalesce(Beverages_amount,0)) as Beverages_amount_sum ,min(coalesce(Beverages_amount,0)) as Beverages_amount_min ,max(coalesce(Beverages_amount,0)) as Beverages_amount_max  ,count(distinct case when coalesce(Beverages_amount,0) > 0 then Period end  ) as Beverages_amount_periods ,AVG(coalesce(Snacks_amount,0)) as Snacks_amount_avg ,SUM(coalesce(Snacks_amount,0)) as Snacks_amount_sum ,min(coalesce(Snacks_amount,0)) as Snacks_amount_min ,max(coalesce(Snacks_amount,0)) as Snacks_amount_max  ,count(distinct case when coalesce(Snacks_amount,0) > 0 then Period end  ) as Snacks_amount_periods ,AVG(coalesce(Condiments_amount,0)) as Condiments_amount_avg ,SUM(coalesce(Condiments_amount,0)) as Condiments_amount_sum ,min(coalesce(Condiments_amount,0)) as Condiments_amount_min ,max(coalesce(Condiments_amount,0)) as Condiments_amount_max  ,count(distinct case when coalesce(Condiments_amount,0) > 0 then Period end  ) as Condiments_amount_periods ,AVG(coalesce(Grains_amount,0)) as Grains_amount_avg ,SUM(coalesce(Grains_amount,0)) as Grains_amount_sum ,min(coalesce(Grains_amount,0)) as Grains_amount_min ,max(coalesce(Grains_amount,0)) as Grains_amount_max  ,count(distinct case when coalesce(Grains_amount,0) > 0 then Period end  ) as Grains_amount_periods '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_derivatives(pVar):\n",
    "    sql = \"\"\n",
    "    for h in pVar:\n",
    "        sql += \",AVG(coalesce(\"+h+\",0)) as \" + h + \"_avg \" + \\\n",
    "               \",SUM(coalesce(\"+h+\",0)) as \" + h + \"_sum \" + \\\n",
    "               \",min(coalesce(\"+h+\",0)) as \" + h + \"_min \" + \\\n",
    "               \",max(coalesce(\"+h+\",0)) as \" + h + \"_max \" + \\\n",
    "               \" ,count(distinct case when coalesce(\"+h+\",0) > 0 then Period end  ) as \" + h + \"_periods \"        \n",
    "    return sql\n",
    "\n",
    "grocery_category_amount_df = spark.sql(\"SELECT * FROM grocery_category_amount limit 1\")\n",
    "sql_derivatives = calculate_derivatives([ x for x in grocery_category_amount_df.columns if x.endswith('_amount')])\n",
    "sql_derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e071e3b",
   "metadata": {},
   "source": [
    "We generate a DataFrame called `grouped_grocery_category_amount` by executing an SQL query. In this query, we utilize the previously defined `sql_total_amount` and `sql_derivatives` objects. The results are grouped by the `CLIENT_ID` field, which we declare as `key_field: key_field = 'CLIENT_ID'`.\n",
    "\n",
    "To execute the query, we use the `spark.sql()` function and provide it with a multi-line SQL string. This string includes the `key_field`, `sql_total_amount`, and `sql_derivatives` to select and calculate the total amount and various statistics for each client. The `grocery_category_amount` table serves as the data source for the query.\n",
    "\n",
    "After executing the query, we convert the resulting DataFrame to a Python DataFrame using the `toPandas()` method. This allows us to work with the data in a familiar Python environment.\n",
    "\n",
    "To display all columns of the `python_df2` DataFrame, we use the `pd.set_option()` function from the Pandas library to set the display options for maximum columns. By setting `display.max_columns` to `None`, all columns in the DataFrame will be shown when we print it.\n",
    "\n",
    "Overall, this code demonstrates the utilization of SQL queries in PySpark to group the grocery category amounts by client and calculate various statistics. It further showcases the seamless integration between PySpark and Pandas, allowing us to work with the results in a Python-friendly manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d36b37c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIENT_ID</th>\n",
       "      <th>Bakery_amount_avg</th>\n",
       "      <th>Bakery_amount_sum</th>\n",
       "      <th>Bakery_amount_min</th>\n",
       "      <th>Bakery_amount_max</th>\n",
       "      <th>Bakery_amount_periods</th>\n",
       "      <th>Produce_amount_avg</th>\n",
       "      <th>Produce_amount_sum</th>\n",
       "      <th>Produce_amount_min</th>\n",
       "      <th>Produce_amount_max</th>\n",
       "      <th>Produce_amount_periods</th>\n",
       "      <th>Dairy_amount_avg</th>\n",
       "      <th>Dairy_amount_sum</th>\n",
       "      <th>Dairy_amount_min</th>\n",
       "      <th>Dairy_amount_max</th>\n",
       "      <th>Dairy_amount_periods</th>\n",
       "      <th>Meat_amount_avg</th>\n",
       "      <th>Meat_amount_sum</th>\n",
       "      <th>Meat_amount_min</th>\n",
       "      <th>Meat_amount_max</th>\n",
       "      <th>Meat_amount_periods</th>\n",
       "      <th>Frozen_foods_amount_avg</th>\n",
       "      <th>Frozen_foods_amount_sum</th>\n",
       "      <th>Frozen_foods_amount_min</th>\n",
       "      <th>Frozen_foods_amount_max</th>\n",
       "      <th>Frozen_foods_amount_periods</th>\n",
       "      <th>Canned_goods_amount_avg</th>\n",
       "      <th>Canned_goods_amount_sum</th>\n",
       "      <th>Canned_goods_amount_min</th>\n",
       "      <th>Canned_goods_amount_max</th>\n",
       "      <th>Canned_goods_amount_periods</th>\n",
       "      <th>Beverages_amount_avg</th>\n",
       "      <th>Beverages_amount_sum</th>\n",
       "      <th>Beverages_amount_min</th>\n",
       "      <th>Beverages_amount_max</th>\n",
       "      <th>Beverages_amount_periods</th>\n",
       "      <th>Snacks_amount_avg</th>\n",
       "      <th>Snacks_amount_sum</th>\n",
       "      <th>Snacks_amount_min</th>\n",
       "      <th>Snacks_amount_max</th>\n",
       "      <th>Snacks_amount_periods</th>\n",
       "      <th>Condiments_amount_avg</th>\n",
       "      <th>Condiments_amount_sum</th>\n",
       "      <th>Condiments_amount_min</th>\n",
       "      <th>Condiments_amount_max</th>\n",
       "      <th>Condiments_amount_periods</th>\n",
       "      <th>Grains_amount_avg</th>\n",
       "      <th>Grains_amount_sum</th>\n",
       "      <th>Grains_amount_min</th>\n",
       "      <th>Grains_amount_max</th>\n",
       "      <th>Grains_amount_periods</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>client_3</td>\n",
       "      <td>106.666667</td>\n",
       "      <td>320</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>163.333333</td>\n",
       "      <td>490</td>\n",
       "      <td>120</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>136.666667</td>\n",
       "      <td>410</td>\n",
       "      <td>80</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>156.666667</td>\n",
       "      <td>470</td>\n",
       "      <td>120</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>106.666667</td>\n",
       "      <td>320</td>\n",
       "      <td>80</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>103.333333</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>2</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>420</td>\n",
       "      <td>120</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>480</td>\n",
       "      <td>80</td>\n",
       "      <td>250</td>\n",
       "      <td>3</td>\n",
       "      <td>183.333333</td>\n",
       "      <td>550</td>\n",
       "      <td>150</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>390</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>4160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client_1</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>1</td>\n",
       "      <td>156.666667</td>\n",
       "      <td>470</td>\n",
       "      <td>120</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>76.666667</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>2</td>\n",
       "      <td>143.333333</td>\n",
       "      <td>430</td>\n",
       "      <td>80</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>480</td>\n",
       "      <td>80</td>\n",
       "      <td>250</td>\n",
       "      <td>3</td>\n",
       "      <td>163.333333</td>\n",
       "      <td>490</td>\n",
       "      <td>120</td>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>480</td>\n",
       "      <td>150</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>233.333333</td>\n",
       "      <td>700</td>\n",
       "      <td>150</td>\n",
       "      <td>300</td>\n",
       "      <td>3</td>\n",
       "      <td>146.666667</td>\n",
       "      <td>440</td>\n",
       "      <td>90</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>123.333333</td>\n",
       "      <td>370</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>4240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>client_2</td>\n",
       "      <td>1166.666667</td>\n",
       "      <td>3500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1300</td>\n",
       "      <td>3</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>80</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CLIENT_ID  Bakery_amount_avg  Bakery_amount_sum  Bakery_amount_min  \\\n",
       "0  client_3         106.666667                320                  0   \n",
       "1  client_1          50.000000                150                  0   \n",
       "2  client_2        1166.666667               3500               1000   \n",
       "\n",
       "   Bakery_amount_max  Bakery_amount_periods  Produce_amount_avg  \\\n",
       "0                200                      2          163.333333   \n",
       "1                150                      1          156.666667   \n",
       "2               1300                      3           26.666667   \n",
       "\n",
       "   Produce_amount_sum  Produce_amount_min  Produce_amount_max  \\\n",
       "0                 490                 120                 220   \n",
       "1                 470                 120                 200   \n",
       "2                  80                  10                  40   \n",
       "\n",
       "   Produce_amount_periods  Dairy_amount_avg  Dairy_amount_sum  \\\n",
       "0                       3        136.666667               410   \n",
       "1                       3         76.666667               230   \n",
       "2                       3         33.333333               100   \n",
       "\n",
       "   Dairy_amount_min  Dairy_amount_max  Dairy_amount_periods  Meat_amount_avg  \\\n",
       "0                80               180                     3       156.666667   \n",
       "1                 0               150                     2       143.333333   \n",
       "2                 0                80                     2         0.000000   \n",
       "\n",
       "   Meat_amount_sum  Meat_amount_min  Meat_amount_max  Meat_amount_periods  \\\n",
       "0              470              120              200                    3   \n",
       "1              430               80              200                    3   \n",
       "2                0                0                0                    0   \n",
       "\n",
       "   Frozen_foods_amount_avg  Frozen_foods_amount_sum  Frozen_foods_amount_min  \\\n",
       "0               106.666667                      320                       80   \n",
       "1               160.000000                      480                       80   \n",
       "2                30.000000                       90                        0   \n",
       "\n",
       "   Frozen_foods_amount_max  Frozen_foods_amount_periods  \\\n",
       "0                      150                            3   \n",
       "1                      250                            3   \n",
       "2                       90                            1   \n",
       "\n",
       "   Canned_goods_amount_avg  Canned_goods_amount_sum  Canned_goods_amount_min  \\\n",
       "0               103.333333                      310                        0   \n",
       "1               163.333333                      490                      120   \n",
       "2                 0.000000                        0                        0   \n",
       "\n",
       "   Canned_goods_amount_max  Canned_goods_amount_periods  Beverages_amount_avg  \\\n",
       "0                      160                            2            140.000000   \n",
       "1                      220                            3            160.000000   \n",
       "2                        0                            0             26.666667   \n",
       "\n",
       "   Beverages_amount_sum  Beverages_amount_min  Beverages_amount_max  \\\n",
       "0                   420                   120                   150   \n",
       "1                   480                   150                   180   \n",
       "2                    80                     0                    80   \n",
       "\n",
       "   Beverages_amount_periods  Snacks_amount_avg  Snacks_amount_sum  \\\n",
       "0                         3         160.000000                480   \n",
       "1                         3         233.333333                700   \n",
       "2                         1           0.000000                  0   \n",
       "\n",
       "   Snacks_amount_min  Snacks_amount_max  Snacks_amount_periods  \\\n",
       "0                 80                250                      3   \n",
       "1                150                300                      3   \n",
       "2                  0                  0                      0   \n",
       "\n",
       "   Condiments_amount_avg  Condiments_amount_sum  Condiments_amount_min  \\\n",
       "0             183.333333                    550                    150   \n",
       "1             146.666667                    440                     90   \n",
       "2              26.666667                     80                      0   \n",
       "\n",
       "   Condiments_amount_max  Condiments_amount_periods  Grains_amount_avg  \\\n",
       "0                    200                          3         130.000000   \n",
       "1                    200                          3         123.333333   \n",
       "2                     80                          1           0.000000   \n",
       "\n",
       "   Grains_amount_sum  Grains_amount_min  Grains_amount_max  \\\n",
       "0                390                100                150   \n",
       "1                370                100                150   \n",
       "2                  0                  0                  0   \n",
       "\n",
       "   Grains_amount_periods  total_amount  \n",
       "0                      3          4160  \n",
       "1                      3          4240  \n",
       "2                      0          3930  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "key_field= 'CLIENT_ID'\n",
    "\n",
    "# Use the result in a SQL query\n",
    "grouped_grocery_category_amount = spark.sql(\"\"\"SELECT \"\"\" + key_field + sql_derivatives + sql_total_amount + \"\"\" FROM grocery_category_amount\n",
    "group by \"\"\"+ key_field)\n",
    "\n",
    "# grouped_grocery_category_amount.show(1,vertical=True)\n",
    "\n",
    "# Convert PySpark DataFrame to Python DataFrame\n",
    "python_df2 = grouped_grocery_category_amount.toPandas()\n",
    "pd.set_option('display.max_columns', None)  # Set maximum columns to display\n",
    "python_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854d6fcb",
   "metadata": {},
   "source": [
    "In the provided output, we can see that client_3 has an average spending of 106.66 in the Bakery category over the last 3 periods. The total amount spent in Bakery over the 3 periods was 320. The minimum amount spent in Bakery was 0, and the maximum amount was 200. Client_3 made purchases in the Bakery category in 2 out of the 3 periods.\n",
    "\n",
    "Similarly, client_1 has an average spending of 50 in the Bakery category over the last 3 periods. The total amount spent in Bakery over the 3 periods was 150. The minimum amount spent in Bakery was 0, and the maximum amount was 150. Client_1 made purchases in the Bakery category in 1 out of the 3 periods.\n",
    "\n",
    "Additionally, client_2 has an average spending of 1166.66 in the Bakery category over the last 3 periods. The total amount spent in Bakery over the 3 periods was 3500. The minimum amount spent in Bakery was 1000, and the maximum amount was $1300. Client_2 made purchases in the Bakery category in all 3 periods.\n",
    "\n",
    "These insights provide a summary of the spending behavior for each client in the Bakery category, including average amounts, total spending, minimum and maximum amounts, and the number of periods in which purchases were made. Similar calculations are performed for other categories such as produce, dairy, meat, frozen foods, canned goods, beverages, snacks, condiments, and grains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0d271d",
   "metadata": {},
   "source": [
    "# Feature Engineering: Calculating the Percentages of the Total Amount for Grocery Category Amounts with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2ebda",
   "metadata": {},
   "source": [
    "Before utilizing the `grouped_grocery_category_amount` dataframe in SQL queries, we establish a temporary view named \"grouped_grocery_category_amount\" through the `createOrReplaceTempView` method. This action enables us to reference the dataframe by its designated view name in subsequent SQL queries, simplifying the syntax and enhancing readability.\n",
    "\n",
    "Like the `calculate_total` and `calculate_derivatives` functions, we define the `calculate_percentages` function. This function takes two parameters: `pVar` and `pName`. When calling the function as `calculate_percentages(pVar, pName)`, `pVar` represents a list of variables from the dataframe to which the function will be applied, while `pName` is the name of the total amount field created previously with `calculate_total`.\n",
    "\n",
    "The purpose of this function is to generate SQL code that calculates the percentage of the total amount for each category of all fields in the dataset that end with the suffix \"_sum\". To achieve this, the function iterates through the elements in `pVar` and constructs a partial SQL statement.\n",
    "\n",
    "The resulting SQL code is stored as a string in the `sql_percent_amount` object.\n",
    "\n",
    "In the provided code snippet, we first retrieve the first row of the `grouped_grocery_category_amount_df` dataset using the `spark.sql` function. This dataset likely contains information about grocery category amounts.\n",
    "\n",
    "Then, we call the `calculate_percentages` function with a list comprehension that extracts the column names ending with \"_sum\" from `grouped_grocery_category_amount_df.columns`. The result is assigned to the `sql_percent_amount` variable, which now holds the generated SQL code for calculating the desired percentages.\n",
    "\n",
    "Overall, this code demonstrates the process of generating SQL code to calculate the percentages of the total amount for each category in the dataset based on the summed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2004377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "grouped_grocery_category_amount.createOrReplaceTempView(\"grouped_grocery_category_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3396ccfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', round(Bakery_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Bakery_amount_sum_perc, round(Produce_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Produce_amount_sum_perc, round(Dairy_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Dairy_amount_sum_perc, round(Meat_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Meat_amount_sum_perc, round(Frozen_foods_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Frozen_foods_amount_sum_perc, round(Canned_goods_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Canned_goods_amount_sum_perc, round(Beverages_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Beverages_amount_sum_perc, round(Snacks_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Snacks_amount_sum_perc, round(Condiments_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Condiments_amount_sum_perc, round(Grains_amount_sum / case when total_amount = 0 then 1 else total_amount end, 3) as Grains_amount_sum_perc'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of TOTAL_AMOUNT\n",
    "\n",
    "def calculate_percentages(pVar, pName):\n",
    "    sql2 = \"\"\n",
    "    for h in pVar:\n",
    "        sql2 += \", round(\"+h+\" / case when total_\" + pName + \"\"\" = 0 then 1 else total_\"\"\" + pName + \" end, 3) as \" + h + \"_perc\"\n",
    "    return sql2\n",
    "\n",
    "grouped_grocery_category_amount_df = spark.sql(\"SELECT * FROM grouped_grocery_category_amount limit 1\")\n",
    "sql_percent_amount = calculate_percentages([ x for x in grouped_grocery_category_amount_df.columns if (x.endswith('_sum'))], 'amount')\n",
    "sql_percent_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91946f59",
   "metadata": {},
   "source": [
    "We create a DataFrame named `grouped_grocery_category_amount_V2` by executing an SQL query. In this query, we utilize the previously defined `sql_percent_amount` object. The results are grouped by the `CLIENT_ID` field, which we declare as `key_field: key_field = 'CLIENT_ID'`.\n",
    "\n",
    "To execute the query, we use the `spark.sql()` function and provide it with a multi-line SQL string. This string includes the `key_field` and `sql_percent_amount`. The `grouped_grocery_category_amount` DataFrame serves as the data source for the query.\n",
    "\n",
    "After executing the query, we convert the resulting DataFrame to a Python DataFrame using the `toPandas()` method. This conversion allows us to work with the data in a familiar Python environment and utilize various Python libraries and functions for further analysis and visualization.\n",
    "\n",
    "Overall, this code demonstrates the usage of SQL queries in PySpark to generate a new DataFrame, `grouped_grocery_category_amount_V2`, by calculating the percentages of the total amount for each category. The resulting DataFrame is then converted to a Python DataFrame for easier data manipulation and exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f9bbf52",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIENT_ID</th>\n",
       "      <th>Bakery_amount_sum_perc</th>\n",
       "      <th>Produce_amount_sum_perc</th>\n",
       "      <th>Dairy_amount_sum_perc</th>\n",
       "      <th>Meat_amount_sum_perc</th>\n",
       "      <th>Frozen_foods_amount_sum_perc</th>\n",
       "      <th>Canned_goods_amount_sum_perc</th>\n",
       "      <th>Beverages_amount_sum_perc</th>\n",
       "      <th>Snacks_amount_sum_perc</th>\n",
       "      <th>Condiments_amount_sum_perc</th>\n",
       "      <th>Grains_amount_sum_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>client_1</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client_2</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>client_3</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CLIENT_ID  Bakery_amount_sum_perc  Produce_amount_sum_perc  \\\n",
       "0  client_1                   0.035                    0.111   \n",
       "1  client_2                   0.891                    0.020   \n",
       "2  client_3                   0.077                    0.118   \n",
       "\n",
       "   Dairy_amount_sum_perc  Meat_amount_sum_perc  Frozen_foods_amount_sum_perc  \\\n",
       "0                  0.054                 0.101                         0.113   \n",
       "1                  0.025                 0.000                         0.023   \n",
       "2                  0.099                 0.113                         0.077   \n",
       "\n",
       "   Canned_goods_amount_sum_perc  Beverages_amount_sum_perc  \\\n",
       "0                         0.116                      0.113   \n",
       "1                         0.000                      0.020   \n",
       "2                         0.075                      0.101   \n",
       "\n",
       "   Snacks_amount_sum_perc  Condiments_amount_sum_perc  Grains_amount_sum_perc  \n",
       "0                   0.165                       0.104                   0.087  \n",
       "1                   0.000                       0.020                   0.000  \n",
       "2                   0.115                       0.132                   0.094  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the result in a SQL query\n",
    "grouped_grocery_category_amount_V2 = spark.sql(\"\"\"SELECT \"\"\" + key_field + sql_percent_amount + \"\"\" FROM grouped_grocery_category_amount \"\"\")\n",
    "\n",
    "# Convert PySpark DataFrame to Python DataFrame\n",
    "python_df3 = grouped_grocery_category_amount_V2.toPandas()\n",
    "# pd.set_option('display.max_columns', None)  # Set maximum columns to display\n",
    "python_df3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d63c38",
   "metadata": {},
   "source": [
    "In the provided output, we can observe the spending percentages for client_1, client_2, and client_3 in different categories over the last 3 months.\n",
    "\n",
    "For client_1, we see that they spent approximately 3.5% of their total expenditure in the Bakery category, 1.11% in the Produce category, and allocated portions of their spending to other categories as well. The percentages reflect the proportion of client_1's total expenditure allocated to each category.\n",
    "\n",
    "Similarly, for client_2 and client_3, we can analyze their spending patterns across the different categories. The same logic applies, where we observe the percentages of total spending allocated to each category for client_2 and client_3.\n",
    "\n",
    "These percentages provide valuable insights into the distribution of spending for each client across various categories. It allows us to understand their preferences and patterns of expenditure. Importantly, the sum of these percentages for each client adds up to 100%, indicating that all of their spending has been accounted for across the analyzed categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3e2cab",
   "metadata": {},
   "source": [
    "# Feature Engineering: Calculating the Flag for High Spending Customers in Specific Supermarket Categories (PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da877e",
   "metadata": {},
   "source": [
    "Before using the `grouped_grocery_category_amount_V2` DataFrame in SQL queries, we create a temporary view named \"grouped_grocery_category_amount_V2\" using the `createOrReplaceTempView` method. This step allows us to refer to the DataFrame by its assigned view name in subsequent SQL queries, simplifying the syntax and improving readability.\n",
    "\n",
    "Similar to the `calculate_total`, `calculate_derivatives`, and `calculate_percentages` functions, we also define the `MoreThan80pInCategory` function. This function accepts two parameters: `pVar` and `pName`. When calling the function as `MoreThan80pInCategory(pVar, pName)`, `pVar` represents a list of variables from the DataFrame to which the function will be applied, while `pName` denotes the calculated field name.\n",
    "\n",
    "The purpose of this function is to generate SQL code that calculates the flag (0 or 1) indicating whether a customer spends more than 80% of the total amount in at least one specific supermarket category. This helps identify customers who consistently spend in a particular category or across multiple categories. To accomplish this, the function iterates through the elements in `pVar` and constructs a partial SQL statement.\n",
    "\n",
    "The resulting SQL code is stored as a string in the `sql_MoreThan80pInCategory` object.\n",
    "\n",
    "In the provided code snippet, we first retrieve the first row of the `grouped_grocery_category_amount_V2` dataset using the `spark.sql` function.\n",
    "\n",
    "Next, we invoke the `MoreThan80pInCategory` function with a list comprehension that extracts the column names ending with \"_perc\" from `grouped_grocery_category_amount_V2.columns`. The generated SQL code for calculating the flag is assigned to the `sql_MoreThan80pInCategory` variable.\n",
    "\n",
    "Overall, this code demonstrates the process of generating SQL code to calculate the flag variable for each category in the dataset based on customers' spending patterns, particularly whether they exceed 80% of the total amount in at least one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4301eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view\n",
    "grouped_grocery_category_amount_V2.createOrReplaceTempView(\"grouped_grocery_category_amount_V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab07baa7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', CASE WHEN  case when Bakery_amount_sum_perc > 0.8 then 1 else 0 end +  case when Produce_amount_sum_perc > 0.8 then 1 else 0 end +  case when Dairy_amount_sum_perc > 0.8 then 1 else 0 end +  case when Meat_amount_sum_perc > 0.8 then 1 else 0 end +  case when Frozen_foods_amount_sum_perc > 0.8 then 1 else 0 end +  case when Canned_goods_amount_sum_perc > 0.8 then 1 else 0 end +  case when Beverages_amount_sum_perc > 0.8 then 1 else 0 end +  case when Snacks_amount_sum_perc > 0.8 then 1 else 0 end +  case when Condiments_amount_sum_perc > 0.8 then 1 else 0 end +  case when Grains_amount_sum_perc > 0.8 then 1 else 0 end>0 THEN 1 ELSE 0 END as FLAG_MoreThan80p_amount'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flag Customers consuming more than 80% in at least one category\n",
    "\n",
    "def MoreThan80pInCategory(pVar, pName):\n",
    "    sql = \", CASE WHEN \"\n",
    "    for p in pVar:\n",
    "        pp = \" case when \" + p + \" > 0.8 then 1 else 0 end\"\n",
    "        if sql == \", CASE WHEN \":\n",
    "            sql += pp\n",
    "        else:\n",
    "            sql += \" + \" + pp\n",
    "    sql += \">0 THEN 1 ELSE 0 END as FLAG_MoreThan80p_\" + pName\n",
    "    return sql\n",
    "\n",
    "grouped_grocery_category_amount_V2_df = spark.sql(\"SELECT * FROM grouped_grocery_category_amount_V2 limit 1\")\n",
    "sql_MoreThan80pInCategory = MoreThan80pInCategory([ x for x in grouped_grocery_category_amount_V2.columns if (x.endswith('_perc'))], 'amount')\n",
    "sql_MoreThan80pInCategory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71db2241",
   "metadata": {},
   "source": [
    "We generate a DataFrame called `grouped_grocery_category_amount_V3` by executing an SQL query. This query utilizes the previously defined `sql_MoreThan80pInCategory` object. The query groups the data by the `CLIENT_ID` field, which we declare as `key_field: key_field = 'CLIENT_ID'`.\n",
    "\n",
    "To execute the query, we use the `spark.sql()` function and provide it with a multi-line SQL string. This string includes the `key_field` and `sql_MoreThan80pInCategory`. The `grouped_grocery_category_amount_V2` DataFrame serves as the data source for the query.\n",
    "\n",
    "After executing the query, we convert the resulting DataFrame to a Python DataFrame using the `toPandas()` method. This conversion allows us to work with the data in a familiar Python environment and leverage various Python libraries and functions for further analysis and visualization.\n",
    "\n",
    "Overall, this code demonstrates the utilization of SQL queries in PySpark to generate a new DataFrame, `grouped_grocery_category_amount_V3`, by calculating the flag (0 or 1) indicating whether a customer spends more than 80% of the total amount in at least one specific supermarket category. The resulting DataFrame is then converted to a Python DataFrame for easier data manipulation and exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af104c11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLIENT_ID</th>\n",
       "      <th>FLAG_MoreThan80p_amount</th>\n",
       "      <th>CLIENT_ID</th>\n",
       "      <th>Bakery_amount_sum_perc</th>\n",
       "      <th>Produce_amount_sum_perc</th>\n",
       "      <th>Dairy_amount_sum_perc</th>\n",
       "      <th>Meat_amount_sum_perc</th>\n",
       "      <th>Frozen_foods_amount_sum_perc</th>\n",
       "      <th>Canned_goods_amount_sum_perc</th>\n",
       "      <th>Beverages_amount_sum_perc</th>\n",
       "      <th>Snacks_amount_sum_perc</th>\n",
       "      <th>Condiments_amount_sum_perc</th>\n",
       "      <th>Grains_amount_sum_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>client_1</td>\n",
       "      <td>0</td>\n",
       "      <td>client_1</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>client_2</td>\n",
       "      <td>1</td>\n",
       "      <td>client_2</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>client_3</td>\n",
       "      <td>0</td>\n",
       "      <td>client_3</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CLIENT_ID  FLAG_MoreThan80p_amount CLIENT_ID  Bakery_amount_sum_perc  \\\n",
       "0  client_1                        0  client_1                   0.035   \n",
       "1  client_2                        1  client_2                   0.891   \n",
       "2  client_3                        0  client_3                   0.077   \n",
       "\n",
       "   Produce_amount_sum_perc  Dairy_amount_sum_perc  Meat_amount_sum_perc  \\\n",
       "0                    0.111                  0.054                 0.101   \n",
       "1                    0.020                  0.025                 0.000   \n",
       "2                    0.118                  0.099                 0.113   \n",
       "\n",
       "   Frozen_foods_amount_sum_perc  Canned_goods_amount_sum_perc  \\\n",
       "0                         0.113                         0.116   \n",
       "1                         0.023                         0.000   \n",
       "2                         0.077                         0.075   \n",
       "\n",
       "   Beverages_amount_sum_perc  Snacks_amount_sum_perc  \\\n",
       "0                      0.113                   0.165   \n",
       "1                      0.020                   0.000   \n",
       "2                      0.101                   0.115   \n",
       "\n",
       "   Condiments_amount_sum_perc  Grains_amount_sum_perc  \n",
       "0                       0.104                   0.087  \n",
       "1                       0.020                   0.000  \n",
       "2                       0.132                   0.094  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the result in a SQL query\n",
    "grouped_grocery_category_amount_V3 = spark.sql(\"\"\"SELECT \"\"\" + key_field + sql_MoreThan80pInCategory + \"\"\",a.* FROM grouped_grocery_category_amount_V2 a \"\"\")\n",
    "# Convert PySpark DataFrame to Python DataFrame\n",
    "python_df4 = grouped_grocery_category_amount_V3.toPandas()\n",
    "# pd.set_option('display.max_columns', None)  # Set maximum columns to display\n",
    "python_df4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f2da39",
   "metadata": {},
   "source": [
    "As we can see in the table above, client 2 has a FLAG_MoreThan80p_amount = 1 because this client spends 89% of their total expenditure in the Bakery category over the last 3 periods or months."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d3a9b3",
   "metadata": {},
   "source": [
    "# Feature Engineering: Generating Ratio Variables for Grocery Category Amounts with PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2a557",
   "metadata": {},
   "source": [
    "Finally, we can generate ratio variables. Similar to the `calculate_total`, `calculate_derivatives`, `calculate_percentages`, and `MoreThan80pInCategory` functions, we also define the `CalculateRatioComparison` function. This function accepts one parameter: `pVar`. When calling the function as `CalculateRatioComparison(pVar)`, `pVar` represents a list of variables from the DataFrame to which the function will be applied.\n",
    "\n",
    "The purpose of this function is to generate SQL code that calculates the ratio between supermarket categories. This helps in creating interaction variables. To achieve this, the function iterates through the elements in `pVar` and constructs a partial SQL statement.\n",
    "\n",
    "The resulting SQL code is stored as a string in the `sql_ratio_amount` object.\n",
    "\n",
    "In the provided code snippet, we first retrieve the first row of the `grouped_grocery_category_amount` dataset using the `spark.sql` function.\n",
    "\n",
    "Next, we invoke the `CalculateRatioComparison` function with a list comprehension that extracts the column names ending with \"_sum\" from `grouped_grocery_category_amount.columns`. This generates the SQL code for calculating the ratio between supermarket categories.\n",
    "\n",
    "Overall, this code demonstrates the process of generating SQL code to calculate the ratio variables for each category in the dataset based on their summed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b2cc148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', round(Bakery_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Bakery_amount_sum_vs_Bakery_amount_sum, round(Bakery_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Bakery_amount_sum_vs_Produce_amount_sum, round(Bakery_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Bakery_amount_sum_vs_Dairy_amount_sum, round(Bakery_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Bakery_amount_sum_vs_Meat_amount_sum, round(Bakery_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Bakery_amount_sum_vs_Frozen_foods_amount_sum, round(Bakery_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Bakery_amount_sum_vs_Canned_goods_amount_sum, round(Bakery_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Bakery_amount_sum_vs_Beverages_amount_sum, round(Bakery_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Bakery_amount_sum_vs_Snacks_amount_sum, round(Bakery_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Bakery_amount_sum_vs_Condiments_amount_sum, round(Bakery_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Bakery_amount_sum_vs_Grains_amount_sum, round(Produce_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Produce_amount_sum_vs_Bakery_amount_sum, round(Produce_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Produce_amount_sum_vs_Produce_amount_sum, round(Produce_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Produce_amount_sum_vs_Dairy_amount_sum, round(Produce_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Produce_amount_sum_vs_Meat_amount_sum, round(Produce_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Produce_amount_sum_vs_Frozen_foods_amount_sum, round(Produce_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Produce_amount_sum_vs_Canned_goods_amount_sum, round(Produce_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Produce_amount_sum_vs_Beverages_amount_sum, round(Produce_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Produce_amount_sum_vs_Snacks_amount_sum, round(Produce_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Produce_amount_sum_vs_Condiments_amount_sum, round(Produce_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Produce_amount_sum_vs_Grains_amount_sum, round(Dairy_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Dairy_amount_sum_vs_Bakery_amount_sum, round(Dairy_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Dairy_amount_sum_vs_Produce_amount_sum, round(Dairy_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Dairy_amount_sum_vs_Dairy_amount_sum, round(Dairy_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Dairy_amount_sum_vs_Meat_amount_sum, round(Dairy_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Dairy_amount_sum_vs_Frozen_foods_amount_sum, round(Dairy_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Dairy_amount_sum_vs_Canned_goods_amount_sum, round(Dairy_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Dairy_amount_sum_vs_Beverages_amount_sum, round(Dairy_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Dairy_amount_sum_vs_Snacks_amount_sum, round(Dairy_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Dairy_amount_sum_vs_Condiments_amount_sum, round(Dairy_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Dairy_amount_sum_vs_Grains_amount_sum, round(Meat_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Meat_amount_sum_vs_Bakery_amount_sum, round(Meat_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Meat_amount_sum_vs_Produce_amount_sum, round(Meat_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Meat_amount_sum_vs_Dairy_amount_sum, round(Meat_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Meat_amount_sum_vs_Meat_amount_sum, round(Meat_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Meat_amount_sum_vs_Frozen_foods_amount_sum, round(Meat_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Meat_amount_sum_vs_Canned_goods_amount_sum, round(Meat_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Meat_amount_sum_vs_Beverages_amount_sum, round(Meat_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Meat_amount_sum_vs_Snacks_amount_sum, round(Meat_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Meat_amount_sum_vs_Condiments_amount_sum, round(Meat_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Meat_amount_sum_vs_Grains_amount_sum, round(Frozen_foods_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Bakery_amount_sum, round(Frozen_foods_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Produce_amount_sum, round(Frozen_foods_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Dairy_amount_sum, round(Frozen_foods_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Meat_amount_sum, round(Frozen_foods_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Frozen_foods_amount_sum, round(Frozen_foods_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Canned_goods_amount_sum, round(Frozen_foods_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Beverages_amount_sum, round(Frozen_foods_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Snacks_amount_sum, round(Frozen_foods_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Condiments_amount_sum, round(Frozen_foods_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Frozen_foods_amount_sum_vs_Grains_amount_sum, round(Canned_goods_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Canned_goods_amount_sum_vs_Bakery_amount_sum, round(Canned_goods_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Canned_goods_amount_sum_vs_Produce_amount_sum, round(Canned_goods_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Canned_goods_amount_sum_vs_Dairy_amount_sum, round(Canned_goods_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Canned_goods_amount_sum_vs_Meat_amount_sum, round(Canned_goods_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Canned_goods_amount_sum_vs_Frozen_foods_amount_sum, round(Canned_goods_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Canned_goods_amount_sum_vs_Canned_goods_amount_sum, round(Canned_goods_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Canned_goods_amount_sum_vs_Beverages_amount_sum, round(Canned_goods_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Canned_goods_amount_sum_vs_Snacks_amount_sum, round(Canned_goods_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Canned_goods_amount_sum_vs_Condiments_amount_sum, round(Canned_goods_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Canned_goods_amount_sum_vs_Grains_amount_sum, round(Beverages_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Beverages_amount_sum_vs_Bakery_amount_sum, round(Beverages_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Beverages_amount_sum_vs_Produce_amount_sum, round(Beverages_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Beverages_amount_sum_vs_Dairy_amount_sum, round(Beverages_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Beverages_amount_sum_vs_Meat_amount_sum, round(Beverages_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Beverages_amount_sum_vs_Frozen_foods_amount_sum, round(Beverages_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Beverages_amount_sum_vs_Canned_goods_amount_sum, round(Beverages_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Beverages_amount_sum_vs_Beverages_amount_sum, round(Beverages_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Beverages_amount_sum_vs_Snacks_amount_sum, round(Beverages_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Beverages_amount_sum_vs_Condiments_amount_sum, round(Beverages_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Beverages_amount_sum_vs_Grains_amount_sum, round(Snacks_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Snacks_amount_sum_vs_Bakery_amount_sum, round(Snacks_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Snacks_amount_sum_vs_Produce_amount_sum, round(Snacks_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Snacks_amount_sum_vs_Dairy_amount_sum, round(Snacks_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Snacks_amount_sum_vs_Meat_amount_sum, round(Snacks_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Snacks_amount_sum_vs_Frozen_foods_amount_sum, round(Snacks_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Snacks_amount_sum_vs_Canned_goods_amount_sum, round(Snacks_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Snacks_amount_sum_vs_Beverages_amount_sum, round(Snacks_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Snacks_amount_sum_vs_Snacks_amount_sum, round(Snacks_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Snacks_amount_sum_vs_Condiments_amount_sum, round(Snacks_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Snacks_amount_sum_vs_Grains_amount_sum, round(Condiments_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Condiments_amount_sum_vs_Bakery_amount_sum, round(Condiments_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Condiments_amount_sum_vs_Produce_amount_sum, round(Condiments_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Condiments_amount_sum_vs_Dairy_amount_sum, round(Condiments_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Condiments_amount_sum_vs_Meat_amount_sum, round(Condiments_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Condiments_amount_sum_vs_Frozen_foods_amount_sum, round(Condiments_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Condiments_amount_sum_vs_Canned_goods_amount_sum, round(Condiments_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Condiments_amount_sum_vs_Beverages_amount_sum, round(Condiments_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Condiments_amount_sum_vs_Snacks_amount_sum, round(Condiments_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Condiments_amount_sum_vs_Condiments_amount_sum, round(Condiments_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Condiments_amount_sum_vs_Grains_amount_sum, round(Grains_amount_sum / case when Bakery_amount_sum = 0 then 1 else Bakery_amount_sum end, 3) as Grains_amount_sum_vs_Bakery_amount_sum, round(Grains_amount_sum / case when Produce_amount_sum = 0 then 1 else Produce_amount_sum end, 3) as Grains_amount_sum_vs_Produce_amount_sum, round(Grains_amount_sum / case when Dairy_amount_sum = 0 then 1 else Dairy_amount_sum end, 3) as Grains_amount_sum_vs_Dairy_amount_sum, round(Grains_amount_sum / case when Meat_amount_sum = 0 then 1 else Meat_amount_sum end, 3) as Grains_amount_sum_vs_Meat_amount_sum, round(Grains_amount_sum / case when Frozen_foods_amount_sum = 0 then 1 else Frozen_foods_amount_sum end, 3) as Grains_amount_sum_vs_Frozen_foods_amount_sum, round(Grains_amount_sum / case when Canned_goods_amount_sum = 0 then 1 else Canned_goods_amount_sum end, 3) as Grains_amount_sum_vs_Canned_goods_amount_sum, round(Grains_amount_sum / case when Beverages_amount_sum = 0 then 1 else Beverages_amount_sum end, 3) as Grains_amount_sum_vs_Beverages_amount_sum, round(Grains_amount_sum / case when Snacks_amount_sum = 0 then 1 else Snacks_amount_sum end, 3) as Grains_amount_sum_vs_Snacks_amount_sum, round(Grains_amount_sum / case when Condiments_amount_sum = 0 then 1 else Condiments_amount_sum end, 3) as Grains_amount_sum_vs_Condiments_amount_sum, round(Grains_amount_sum / case when Grains_amount_sum = 0 then 1 else Grains_amount_sum end, 3) as Grains_amount_sum_vs_Grains_amount_sum'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CalculateRatioComparison(pVar):\n",
    "    sql = \"\"\n",
    "    for h in pVar:\n",
    "          for h2 in pVar:\n",
    "                sql += \", round(\"+h+\" / case when \" + h2 + \" = 0 then 1 else \" + h2 + \" end, 3) as \" + h + \"_vs_\" + h2\n",
    "    return  sql\n",
    "\n",
    "grouped_grocery_category_amount_df = spark.sql(\"SELECT * FROM grouped_grocery_category_amount limit 1\")\n",
    "sql_ratio_amount = CalculateRatioComparison([ x for x in grouped_grocery_category_amount.columns if (x.endswith('_sum'))])\n",
    "sql_ratio_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e9f6a",
   "metadata": {},
   "source": [
    "We generate a DataFrame called `grouped_grocery_category_amount_V4` by executing an SQL query. This query utilizes the previously defined `sql_ratio_amount` object. The query groups the data by the `CLIENT_ID` field, which we declare as `key_field`: `key_field = 'CLIENT_ID'`.\n",
    "\n",
    "To execute the query, we use the `spark.sql()` function and provide it with a multi-line SQL string. This string includes the `key_field` and `sql_ratio_amount`. The `grouped_grocery_category_amount` DataFrame serves as the data source for the query.\n",
    "\n",
    "After executing the query, we display the first row of the resulting DataFrame using the `show()` function. The `vertical=True` parameter allows us to display the row vertically, providing a clear view of the calculated ratio variables.\n",
    "\n",
    "Overall, this code demonstrates the usage of SQL queries in PySpark to generate a new DataFrame, `grouped_grocery_category_amount_V4`, by calculating the ratio between supermarket categories. The resulting DataFrame is then displayed to examine the calculated ratio variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a346f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------------------------\n",
      " CLIENT_ID                                          | client_1 \n",
      " Bakery_amount_sum_vs_Bakery_amount_sum             | 1.0      \n",
      " Bakery_amount_sum_vs_Produce_amount_sum            | 0.319    \n",
      " Bakery_amount_sum_vs_Dairy_amount_sum              | 0.652    \n",
      " Bakery_amount_sum_vs_Meat_amount_sum               | 0.349    \n",
      " Bakery_amount_sum_vs_Frozen_foods_amount_sum       | 0.313    \n",
      " Bakery_amount_sum_vs_Canned_goods_amount_sum       | 0.306    \n",
      " Bakery_amount_sum_vs_Beverages_amount_sum          | 0.313    \n",
      " Bakery_amount_sum_vs_Snacks_amount_sum             | 0.214    \n",
      " Bakery_amount_sum_vs_Condiments_amount_sum         | 0.341    \n",
      " Bakery_amount_sum_vs_Grains_amount_sum             | 0.405    \n",
      " Produce_amount_sum_vs_Bakery_amount_sum            | 3.133    \n",
      " Produce_amount_sum_vs_Produce_amount_sum           | 1.0      \n",
      " Produce_amount_sum_vs_Dairy_amount_sum             | 2.043    \n",
      " Produce_amount_sum_vs_Meat_amount_sum              | 1.093    \n",
      " Produce_amount_sum_vs_Frozen_foods_amount_sum      | 0.979    \n",
      " Produce_amount_sum_vs_Canned_goods_amount_sum      | 0.959    \n",
      " Produce_amount_sum_vs_Beverages_amount_sum         | 0.979    \n",
      " Produce_amount_sum_vs_Snacks_amount_sum            | 0.671    \n",
      " Produce_amount_sum_vs_Condiments_amount_sum        | 1.068    \n",
      " Produce_amount_sum_vs_Grains_amount_sum            | 1.27     \n",
      " Dairy_amount_sum_vs_Bakery_amount_sum              | 1.533    \n",
      " Dairy_amount_sum_vs_Produce_amount_sum             | 0.489    \n",
      " Dairy_amount_sum_vs_Dairy_amount_sum               | 1.0      \n",
      " Dairy_amount_sum_vs_Meat_amount_sum                | 0.535    \n",
      " Dairy_amount_sum_vs_Frozen_foods_amount_sum        | 0.479    \n",
      " Dairy_amount_sum_vs_Canned_goods_amount_sum        | 0.469    \n",
      " Dairy_amount_sum_vs_Beverages_amount_sum           | 0.479    \n",
      " Dairy_amount_sum_vs_Snacks_amount_sum              | 0.329    \n",
      " Dairy_amount_sum_vs_Condiments_amount_sum          | 0.523    \n",
      " Dairy_amount_sum_vs_Grains_amount_sum              | 0.622    \n",
      " Meat_amount_sum_vs_Bakery_amount_sum               | 2.867    \n",
      " Meat_amount_sum_vs_Produce_amount_sum              | 0.915    \n",
      " Meat_amount_sum_vs_Dairy_amount_sum                | 1.87     \n",
      " Meat_amount_sum_vs_Meat_amount_sum                 | 1.0      \n",
      " Meat_amount_sum_vs_Frozen_foods_amount_sum         | 0.896    \n",
      " Meat_amount_sum_vs_Canned_goods_amount_sum         | 0.878    \n",
      " Meat_amount_sum_vs_Beverages_amount_sum            | 0.896    \n",
      " Meat_amount_sum_vs_Snacks_amount_sum               | 0.614    \n",
      " Meat_amount_sum_vs_Condiments_amount_sum           | 0.977    \n",
      " Meat_amount_sum_vs_Grains_amount_sum               | 1.162    \n",
      " Frozen_foods_amount_sum_vs_Bakery_amount_sum       | 3.2      \n",
      " Frozen_foods_amount_sum_vs_Produce_amount_sum      | 1.021    \n",
      " Frozen_foods_amount_sum_vs_Dairy_amount_sum        | 2.087    \n",
      " Frozen_foods_amount_sum_vs_Meat_amount_sum         | 1.116    \n",
      " Frozen_foods_amount_sum_vs_Frozen_foods_amount_sum | 1.0      \n",
      " Frozen_foods_amount_sum_vs_Canned_goods_amount_sum | 0.98     \n",
      " Frozen_foods_amount_sum_vs_Beverages_amount_sum    | 1.0      \n",
      " Frozen_foods_amount_sum_vs_Snacks_amount_sum       | 0.686    \n",
      " Frozen_foods_amount_sum_vs_Condiments_amount_sum   | 1.091    \n",
      " Frozen_foods_amount_sum_vs_Grains_amount_sum       | 1.297    \n",
      " Canned_goods_amount_sum_vs_Bakery_amount_sum       | 3.267    \n",
      " Canned_goods_amount_sum_vs_Produce_amount_sum      | 1.043    \n",
      " Canned_goods_amount_sum_vs_Dairy_amount_sum        | 2.13     \n",
      " Canned_goods_amount_sum_vs_Meat_amount_sum         | 1.14     \n",
      " Canned_goods_amount_sum_vs_Frozen_foods_amount_sum | 1.021    \n",
      " Canned_goods_amount_sum_vs_Canned_goods_amount_sum | 1.0      \n",
      " Canned_goods_amount_sum_vs_Beverages_amount_sum    | 1.021    \n",
      " Canned_goods_amount_sum_vs_Snacks_amount_sum       | 0.7      \n",
      " Canned_goods_amount_sum_vs_Condiments_amount_sum   | 1.114    \n",
      " Canned_goods_amount_sum_vs_Grains_amount_sum       | 1.324    \n",
      " Beverages_amount_sum_vs_Bakery_amount_sum          | 3.2      \n",
      " Beverages_amount_sum_vs_Produce_amount_sum         | 1.021    \n",
      " Beverages_amount_sum_vs_Dairy_amount_sum           | 2.087    \n",
      " Beverages_amount_sum_vs_Meat_amount_sum            | 1.116    \n",
      " Beverages_amount_sum_vs_Frozen_foods_amount_sum    | 1.0      \n",
      " Beverages_amount_sum_vs_Canned_goods_amount_sum    | 0.98     \n",
      " Beverages_amount_sum_vs_Beverages_amount_sum       | 1.0      \n",
      " Beverages_amount_sum_vs_Snacks_amount_sum          | 0.686    \n",
      " Beverages_amount_sum_vs_Condiments_amount_sum      | 1.091    \n",
      " Beverages_amount_sum_vs_Grains_amount_sum          | 1.297    \n",
      " Snacks_amount_sum_vs_Bakery_amount_sum             | 4.667    \n",
      " Snacks_amount_sum_vs_Produce_amount_sum            | 1.489    \n",
      " Snacks_amount_sum_vs_Dairy_amount_sum              | 3.043    \n",
      " Snacks_amount_sum_vs_Meat_amount_sum               | 1.628    \n",
      " Snacks_amount_sum_vs_Frozen_foods_amount_sum       | 1.458    \n",
      " Snacks_amount_sum_vs_Canned_goods_amount_sum       | 1.429    \n",
      " Snacks_amount_sum_vs_Beverages_amount_sum          | 1.458    \n",
      " Snacks_amount_sum_vs_Snacks_amount_sum             | 1.0      \n",
      " Snacks_amount_sum_vs_Condiments_amount_sum         | 1.591    \n",
      " Snacks_amount_sum_vs_Grains_amount_sum             | 1.892    \n",
      " Condiments_amount_sum_vs_Bakery_amount_sum         | 2.933    \n",
      " Condiments_amount_sum_vs_Produce_amount_sum        | 0.936    \n",
      " Condiments_amount_sum_vs_Dairy_amount_sum          | 1.913    \n",
      " Condiments_amount_sum_vs_Meat_amount_sum           | 1.023    \n",
      " Condiments_amount_sum_vs_Frozen_foods_amount_sum   | 0.917    \n",
      " Condiments_amount_sum_vs_Canned_goods_amount_sum   | 0.898    \n",
      " Condiments_amount_sum_vs_Beverages_amount_sum      | 0.917    \n",
      " Condiments_amount_sum_vs_Snacks_amount_sum         | 0.629    \n",
      " Condiments_amount_sum_vs_Condiments_amount_sum     | 1.0      \n",
      " Condiments_amount_sum_vs_Grains_amount_sum         | 1.189    \n",
      " Grains_amount_sum_vs_Bakery_amount_sum             | 2.467    \n",
      " Grains_amount_sum_vs_Produce_amount_sum            | 0.787    \n",
      " Grains_amount_sum_vs_Dairy_amount_sum              | 1.609    \n",
      " Grains_amount_sum_vs_Meat_amount_sum               | 0.86     \n",
      " Grains_amount_sum_vs_Frozen_foods_amount_sum       | 0.771    \n",
      " Grains_amount_sum_vs_Canned_goods_amount_sum       | 0.755    \n",
      " Grains_amount_sum_vs_Beverages_amount_sum          | 0.771    \n",
      " Grains_amount_sum_vs_Snacks_amount_sum             | 0.529    \n",
      " Grains_amount_sum_vs_Condiments_amount_sum         | 0.841    \n",
      " Grains_amount_sum_vs_Grains_amount_sum             | 1.0      \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the result in a SQL query\n",
    "grouped_grocery_category_amount_V4 = spark.sql(\"\"\"SELECT \"\"\" + key_field + sql_ratio_amount + \"\"\" FROM grouped_grocery_category_amount \"\"\")\n",
    "grouped_grocery_category_amount_V4.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924a8207",
   "metadata": {},
   "source": [
    "In the output above, we can see the ratios calculated for client_1 over the last 3 months. For client_1, the ratio between Bakery and Produce is 0.319, indicating that the amount spent on Bakery is approximately 31.9% of the amount spent on Produce during this period.\n",
    "\n",
    "Similarly, the ratio between Bakery and Dairy is 0.652, implying that the amount spent on Bakery is about 65.2% of the amount spent on Dairy during this period.\n",
    "\n",
    "Lastly, the ratio between Bakery and Meat is 0.349, suggesting that the amount spent on Bakery is approximately 34.9% of the amount spent on Meat during this period.\n",
    "\n",
    "These ratios provide insights into the relative distribution of spending between Bakery and other categories for client_1. The same ratios have also been calculated for client_2 and client_3, allowing us to analyze their spending patterns and compare them across the same categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be743a",
   "metadata": {},
   "source": [
    "# Feature Engineering: Final Dataset with All Created Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d57aa6",
   "metadata": {},
   "source": [
    "In this section, we performed the feature engineering process to create the final dataset called \"grouped_grocery_category_amount_final.\" This dataset includes all the variables we generated throughout the project using data manipulation techniques in PySpark and SQL.\n",
    "\n",
    "The code used to create the final dataset is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cccf41c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------\n",
      " CLIENT_ID                                          | client_3           \n",
      " Bakery_amount_avg                                  | 106.66666666666667 \n",
      " Bakery_amount_sum                                  | 320                \n",
      " Bakery_amount_min                                  | 0                  \n",
      " Bakery_amount_max                                  | 200                \n",
      " Bakery_amount_periods                              | 2                  \n",
      " Produce_amount_avg                                 | 163.33333333333334 \n",
      " Produce_amount_sum                                 | 490                \n",
      " Produce_amount_min                                 | 120                \n",
      " Produce_amount_max                                 | 220                \n",
      " Produce_amount_periods                             | 3                  \n",
      " Dairy_amount_avg                                   | 136.66666666666666 \n",
      " Dairy_amount_sum                                   | 410                \n",
      " Dairy_amount_min                                   | 80                 \n",
      " Dairy_amount_max                                   | 180                \n",
      " Dairy_amount_periods                               | 3                  \n",
      " Meat_amount_avg                                    | 156.66666666666666 \n",
      " Meat_amount_sum                                    | 470                \n",
      " Meat_amount_min                                    | 120                \n",
      " Meat_amount_max                                    | 200                \n",
      " Meat_amount_periods                                | 3                  \n",
      " Frozen_foods_amount_avg                            | 106.66666666666667 \n",
      " Frozen_foods_amount_sum                            | 320                \n",
      " Frozen_foods_amount_min                            | 80                 \n",
      " Frozen_foods_amount_max                            | 150                \n",
      " Frozen_foods_amount_periods                        | 3                  \n",
      " Canned_goods_amount_avg                            | 103.33333333333333 \n",
      " Canned_goods_amount_sum                            | 310                \n",
      " Canned_goods_amount_min                            | 0                  \n",
      " Canned_goods_amount_max                            | 160                \n",
      " Canned_goods_amount_periods                        | 2                  \n",
      " Beverages_amount_avg                               | 140.0              \n",
      " Beverages_amount_sum                               | 420                \n",
      " Beverages_amount_min                               | 120                \n",
      " Beverages_amount_max                               | 150                \n",
      " Beverages_amount_periods                           | 3                  \n",
      " Snacks_amount_avg                                  | 160.0              \n",
      " Snacks_amount_sum                                  | 480                \n",
      " Snacks_amount_min                                  | 80                 \n",
      " Snacks_amount_max                                  | 250                \n",
      " Snacks_amount_periods                              | 3                  \n",
      " Condiments_amount_avg                              | 183.33333333333334 \n",
      " Condiments_amount_sum                              | 550                \n",
      " Condiments_amount_min                              | 150                \n",
      " Condiments_amount_max                              | 200                \n",
      " Condiments_amount_periods                          | 3                  \n",
      " Grains_amount_avg                                  | 130.0              \n",
      " Grains_amount_sum                                  | 390                \n",
      " Grains_amount_min                                  | 100                \n",
      " Grains_amount_max                                  | 150                \n",
      " Grains_amount_periods                              | 3                  \n",
      " total_amount                                       | 4160               \n",
      " Bakery_amount_sum_vs_Bakery_amount_sum             | 1.0                \n",
      " Bakery_amount_sum_vs_Produce_amount_sum            | 0.653              \n",
      " Bakery_amount_sum_vs_Dairy_amount_sum              | 0.78               \n",
      " Bakery_amount_sum_vs_Meat_amount_sum               | 0.681              \n",
      " Bakery_amount_sum_vs_Frozen_foods_amount_sum       | 1.0                \n",
      " Bakery_amount_sum_vs_Canned_goods_amount_sum       | 1.032              \n",
      " Bakery_amount_sum_vs_Beverages_amount_sum          | 0.762              \n",
      " Bakery_amount_sum_vs_Snacks_amount_sum             | 0.667              \n",
      " Bakery_amount_sum_vs_Condiments_amount_sum         | 0.582              \n",
      " Bakery_amount_sum_vs_Grains_amount_sum             | 0.821              \n",
      " Produce_amount_sum_vs_Bakery_amount_sum            | 1.531              \n",
      " Produce_amount_sum_vs_Produce_amount_sum           | 1.0                \n",
      " Produce_amount_sum_vs_Dairy_amount_sum             | 1.195              \n",
      " Produce_amount_sum_vs_Meat_amount_sum              | 1.043              \n",
      " Produce_amount_sum_vs_Frozen_foods_amount_sum      | 1.531              \n",
      " Produce_amount_sum_vs_Canned_goods_amount_sum      | 1.581              \n",
      " Produce_amount_sum_vs_Beverages_amount_sum         | 1.167              \n",
      " Produce_amount_sum_vs_Snacks_amount_sum            | 1.021              \n",
      " Produce_amount_sum_vs_Condiments_amount_sum        | 0.891              \n",
      " Produce_amount_sum_vs_Grains_amount_sum            | 1.256              \n",
      " Dairy_amount_sum_vs_Bakery_amount_sum              | 1.281              \n",
      " Dairy_amount_sum_vs_Produce_amount_sum             | 0.837              \n",
      " Dairy_amount_sum_vs_Dairy_amount_sum               | 1.0                \n",
      " Dairy_amount_sum_vs_Meat_amount_sum                | 0.872              \n",
      " Dairy_amount_sum_vs_Frozen_foods_amount_sum        | 1.281              \n",
      " Dairy_amount_sum_vs_Canned_goods_amount_sum        | 1.323              \n",
      " Dairy_amount_sum_vs_Beverages_amount_sum           | 0.976              \n",
      " Dairy_amount_sum_vs_Snacks_amount_sum              | 0.854              \n",
      " Dairy_amount_sum_vs_Condiments_amount_sum          | 0.745              \n",
      " Dairy_amount_sum_vs_Grains_amount_sum              | 1.051              \n",
      " Meat_amount_sum_vs_Bakery_amount_sum               | 1.469              \n",
      " Meat_amount_sum_vs_Produce_amount_sum              | 0.959              \n",
      " Meat_amount_sum_vs_Dairy_amount_sum                | 1.146              \n",
      " Meat_amount_sum_vs_Meat_amount_sum                 | 1.0                \n",
      " Meat_amount_sum_vs_Frozen_foods_amount_sum         | 1.469              \n",
      " Meat_amount_sum_vs_Canned_goods_amount_sum         | 1.516              \n",
      " Meat_amount_sum_vs_Beverages_amount_sum            | 1.119              \n",
      " Meat_amount_sum_vs_Snacks_amount_sum               | 0.979              \n",
      " Meat_amount_sum_vs_Condiments_amount_sum           | 0.855              \n",
      " Meat_amount_sum_vs_Grains_amount_sum               | 1.205              \n",
      " Frozen_foods_amount_sum_vs_Bakery_amount_sum       | 1.0                \n",
      " Frozen_foods_amount_sum_vs_Produce_amount_sum      | 0.653              \n",
      " Frozen_foods_amount_sum_vs_Dairy_amount_sum        | 0.78               \n",
      " Frozen_foods_amount_sum_vs_Meat_amount_sum         | 0.681              \n",
      " Frozen_foods_amount_sum_vs_Frozen_foods_amount_sum | 1.0                \n",
      " Frozen_foods_amount_sum_vs_Canned_goods_amount_sum | 1.032              \n",
      " Frozen_foods_amount_sum_vs_Beverages_amount_sum    | 0.762              \n",
      " Frozen_foods_amount_sum_vs_Snacks_amount_sum       | 0.667              \n",
      " Frozen_foods_amount_sum_vs_Condiments_amount_sum   | 0.582              \n",
      " Frozen_foods_amount_sum_vs_Grains_amount_sum       | 0.821              \n",
      " Canned_goods_amount_sum_vs_Bakery_amount_sum       | 0.969              \n",
      " Canned_goods_amount_sum_vs_Produce_amount_sum      | 0.633              \n",
      " Canned_goods_amount_sum_vs_Dairy_amount_sum        | 0.756              \n",
      " Canned_goods_amount_sum_vs_Meat_amount_sum         | 0.66               \n",
      " Canned_goods_amount_sum_vs_Frozen_foods_amount_sum | 0.969              \n",
      " Canned_goods_amount_sum_vs_Canned_goods_amount_sum | 1.0                \n",
      " Canned_goods_amount_sum_vs_Beverages_amount_sum    | 0.738              \n",
      " Canned_goods_amount_sum_vs_Snacks_amount_sum       | 0.646              \n",
      " Canned_goods_amount_sum_vs_Condiments_amount_sum   | 0.564              \n",
      " Canned_goods_amount_sum_vs_Grains_amount_sum       | 0.795              \n",
      " Beverages_amount_sum_vs_Bakery_amount_sum          | 1.313              \n",
      " Beverages_amount_sum_vs_Produce_amount_sum         | 0.857              \n",
      " Beverages_amount_sum_vs_Dairy_amount_sum           | 1.024              \n",
      " Beverages_amount_sum_vs_Meat_amount_sum            | 0.894              \n",
      " Beverages_amount_sum_vs_Frozen_foods_amount_sum    | 1.313              \n",
      " Beverages_amount_sum_vs_Canned_goods_amount_sum    | 1.355              \n",
      " Beverages_amount_sum_vs_Beverages_amount_sum       | 1.0                \n",
      " Beverages_amount_sum_vs_Snacks_amount_sum          | 0.875              \n",
      " Beverages_amount_sum_vs_Condiments_amount_sum      | 0.764              \n",
      " Beverages_amount_sum_vs_Grains_amount_sum          | 1.077              \n",
      " Snacks_amount_sum_vs_Bakery_amount_sum             | 1.5                \n",
      " Snacks_amount_sum_vs_Produce_amount_sum            | 0.98               \n",
      " Snacks_amount_sum_vs_Dairy_amount_sum              | 1.171              \n",
      " Snacks_amount_sum_vs_Meat_amount_sum               | 1.021              \n",
      " Snacks_amount_sum_vs_Frozen_foods_amount_sum       | 1.5                \n",
      " Snacks_amount_sum_vs_Canned_goods_amount_sum       | 1.548              \n",
      " Snacks_amount_sum_vs_Beverages_amount_sum          | 1.143              \n",
      " Snacks_amount_sum_vs_Snacks_amount_sum             | 1.0                \n",
      " Snacks_amount_sum_vs_Condiments_amount_sum         | 0.873              \n",
      " Snacks_amount_sum_vs_Grains_amount_sum             | 1.231              \n",
      " Condiments_amount_sum_vs_Bakery_amount_sum         | 1.719              \n",
      " Condiments_amount_sum_vs_Produce_amount_sum        | 1.122              \n",
      " Condiments_amount_sum_vs_Dairy_amount_sum          | 1.341              \n",
      " Condiments_amount_sum_vs_Meat_amount_sum           | 1.17               \n",
      " Condiments_amount_sum_vs_Frozen_foods_amount_sum   | 1.719              \n",
      " Condiments_amount_sum_vs_Canned_goods_amount_sum   | 1.774              \n",
      " Condiments_amount_sum_vs_Beverages_amount_sum      | 1.31               \n",
      " Condiments_amount_sum_vs_Snacks_amount_sum         | 1.146              \n",
      " Condiments_amount_sum_vs_Condiments_amount_sum     | 1.0                \n",
      " Condiments_amount_sum_vs_Grains_amount_sum         | 1.41               \n",
      " Grains_amount_sum_vs_Bakery_amount_sum             | 1.219              \n",
      " Grains_amount_sum_vs_Produce_amount_sum            | 0.796              \n",
      " Grains_amount_sum_vs_Dairy_amount_sum              | 0.951              \n",
      " Grains_amount_sum_vs_Meat_amount_sum               | 0.83               \n",
      " Grains_amount_sum_vs_Frozen_foods_amount_sum       | 1.219              \n",
      " Grains_amount_sum_vs_Canned_goods_amount_sum       | 1.258              \n",
      " Grains_amount_sum_vs_Beverages_amount_sum          | 0.929              \n",
      " Grains_amount_sum_vs_Snacks_amount_sum             | 0.813              \n",
      " Grains_amount_sum_vs_Condiments_amount_sum         | 0.709              \n",
      " Grains_amount_sum_vs_Grains_amount_sum             | 1.0                \n",
      " Bakery_amount_sum_perc                             | 0.077              \n",
      " Produce_amount_sum_perc                            | 0.118              \n",
      " Dairy_amount_sum_perc                              | 0.099              \n",
      " Meat_amount_sum_perc                               | 0.113              \n",
      " Frozen_foods_amount_sum_perc                       | 0.077              \n",
      " Canned_goods_amount_sum_perc                       | 0.075              \n",
      " Beverages_amount_sum_perc                          | 0.101              \n",
      " Snacks_amount_sum_perc                             | 0.115              \n",
      " Condiments_amount_sum_perc                         | 0.132              \n",
      " Grains_amount_sum_perc                             | 0.094              \n",
      " FLAG_MoreThan80p_amount                            | 0                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grouped_grocery_category_amount_final = spark.sql(\"\"\"SELECT grouped_grocery_category_amount.* \"\"\"+ sql_MoreThan80pInCategory\n",
    "+ \"\"\" FROM (SELECT A.* \"\"\"+ sql_ratio_amount  + sql_percent_amount  + \"\"\" FROM (SELECT \"\"\" + key_field + \n",
    "sql_derivatives + sql_total_amount + \"\"\" FROM grocery_category_amount group by \"\"\"+ \n",
    "key_field + \"\"\")A)grouped_grocery_category_amount \"\"\")\n",
    "\n",
    "grouped_grocery_category_amount_final.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f879a0",
   "metadata": {},
   "source": [
    "Above is a preview of the final dataset with one observation.\n",
    "This final dataset contains all the variables generated through the feature engineering process, providing a comprehensive and enriched view of grocery category expenditure data. Its creation enables us to conduct in-depth analysis and build more accurate and effective Data Science models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94ea4c3",
   "metadata": {},
   "source": [
    "# Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c8758b",
   "metadata": {},
   "source": [
    "This data science project showcased the power of advanced feature engineering using PySpark, Python, and SQL. We explored the transformation of a dataset with 10 variables representing different grocery categories, including bakery, produce, dairy, meat, frozen foods, canned goods, beverages, snacks, condiments, and grains. The original table was structured at the CLIENT_ID and Period level, and through advanced feature engineering, we generated a final dataset at the CLIENT_ID level, aggregating data for the last 3 periods.\n",
    "\n",
    "Throughout the project, we achieved the following:\n",
    "\n",
    "- A) Created a new variable representing the \"Total Amount expenditure over the past 3 months,\" aggregating the amounts spent across all grocery categories for each client. This provided a comprehensive overview of their overall spending during the specified period.\n",
    "\n",
    "- B) Calculated various statistics, such as the average (AVG), sum (SUM), minimum (MIN), maximum (MAX), and count of periods where the value is greater than zero for each grocery category. These calculations provided insights into the average, total, and frequency of spending for each category over the last 3 periods.\n",
    "\n",
    "- C) Computed the percentages of the Total Amount for each grocery category, enabling a comparison of the contribution of each category to the overall spending. This analysis shed light on the relative importance of different grocery categories for each client within the specified timeframe.\n",
    "\n",
    "- D) Determined a flag for high spending customers in specific supermarket categories. By setting a threshold, we identified individuals who consistently spent above that threshold in particular categories, revealing their significant purchasing behavior.\n",
    "\n",
    "- E) Generated ratio variables for the grocery categories, allowing for comparisons between categories. These ratios provided insights into the proportion of spending allocated to each grocery category and helped identify potential relationships or trends.\n",
    "\n",
    "By leveraging advanced feature engineering techniques, we transformed the original dataset into a more informative and valuable resource for analysis at the CLIENT_ID level, considering the last 3 periods. The generated variables captured important patterns and behaviors related to grocery category spending, empowering data-driven decision-making and enhancing predictive modeling accuracy.\n",
    "\n",
    "One notable quality of these derived variables is their stability over time. By considering data from the last 3 months, we capture more consistent trends and patterns in customer behavior, providing more robust features for our data science model.\n",
    "\n",
    "Furthermore, these derived variables allow us to incorporate the temporal aspect of the data. By analyzing patterns and trends over the last 3 months, we can capture short-term changes in customer preferences and adjust our models accordingly.\n",
    "\n",
    "In conclusion, this project highlights the importance of advanced feature engineering in data science. By utilizing PySpark, Python, and SQL, we demonstrated how to leverage these techniques to extract insights and create valuable variables from grocery category spending data. These techniques enable data scientists to uncover hidden patterns, understand customer preferences, and make informed business decisions based on a comprehensive analysis of their data, considering the CLIENT_ID level and aggregating data for the last 3 periods. The derived variables offer stability over time and incorporate the temporal aspect of the data, enhancing the accuracy and effectiveness of our data science models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
